{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Config (GPU and Location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU ID can be 0 to number_of_gpus - 1. If you want CPU put -1 \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\";\n",
    "\n",
    "#If you are working in Colab put this variable to False\n",
    "SERVER = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import interact, IntSlider\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "if not SERVER:\n",
    "    !pip -q install -U nltk==3.4.5\n",
    "    %tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the GPU Memory Growth with the use to True and log device to False\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ours and 3rd Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SERVER:\n",
    "    !git pull\n",
    "else:\n",
    "    # Py archives\n",
    "    !git clone https://github.com/JefeLitman/SignLanguageTranslation_SLT.git\n",
    "    # Arrange files\n",
    "    !mv /content/SignLanguageTranslation_SLT/utils /content/\n",
    "    !mv /content/SignLanguageTranslation_SLT/models /content/\n",
    "    !mv /content/SignLanguageTranslation_SLT/metrics /content/\n",
    "    # Delete the remaining files\n",
    "    !rm -rf /content/SignLanguageTranslation_SLT\n",
    "#DatasetsLoaderUtils\n",
    "!wget -q https://raw.githubusercontent.com/JefeLitman/VideoDataGenerator/master/DatasetsLoaderUtils.py -O DatasetsLoaderUtils.py\n",
    "!mv DatasetsLoaderUtils.py utils/DatasetsLoaderUtils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocess_data import preprocessing_paths, preprocessing_sentences, table_paths_dataset\n",
    "from utils.DatasetsLoaderUtils import flow_from_tablePaths\n",
    "from utils.results import save_predictions, calculate_metrics_results\n",
    "from metrics.losses import SparseCategoricalCrossentropy_mask\n",
    "from metrics.accuracy import real_acc\n",
    "from models import compute_features, encoder, decoder, reduce_features, self_attentions, st_attentions, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SERVER:\n",
    "    !rm -rf /content/sample_data\n",
    "    # Download the data\n",
    "    !wget --quiet --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Ph_Ys3O_vI93WeTkDqr5h6kTJm0CZ0Ub' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Ph_Ys3O_vI93WeTkDqr5h6kTJm0CZ0Ub\" -O boston201.zip && rm -rf /tmp/cookies.txt\n",
    "    !unzip -q boston201.zip\n",
    "    !rm boston201.zip\n",
    "    !wget -q https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip -O word_vectors.zip\n",
    "    !unzip -q word_vectors.zip\n",
    "    !rm word_vectors.zip\n",
    "    # Mount drive to save models and results\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model SLT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = namedtuple('Args', 'max_len_sentence data pretrained prefetch_batch_buffer unitsEmbedding vocab_size nIters videos_path rnnUnits dropout recurrent_dropout inputShape optimizer type_frames batchSize epochs lr momentum decay wDecay path2save name')\n",
    "\n",
    "args = Args(max_len_sentence=15,\n",
    "            videos_path='../DataSets/boston201',#'/content/boston201',\n",
    "            rnnUnits=256,\n",
    "            unitsEmbedding=300,\n",
    "            vocab_size=150,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2,\n",
    "            inputShape=(32, 112, 112, 3),\n",
    "            pretrained=None,#'vgg16',\n",
    "            optimizer='adam',\n",
    "            type_frames='jpg/',\n",
    "            batchSize=1,\n",
    "            epochs=20,\n",
    "            nIters=10.0,\n",
    "            lr=0.001,\n",
    "            momentum=0.99,\n",
    "            decay=0.1,\n",
    "            wDecay=0.0005,\n",
    "            path2save='../Saved_Models/', #'/content/drive/My Drive/Models/SLT/<experiment_folder>'\n",
    "            name='SLT_Model',\n",
    "            data= '../DataSets/boston201/data/', #'/content/boston201/data/',\n",
    "            prefetch_batch_buffer = 5\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the seeds for replicability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(8128)\n",
    "np.random.seed(8128)\n",
    "tf.random.set_seed(8128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_translation = [args.data+'translations.train',  \n",
    "                         args.data+'translations.test']\n",
    "paths_videos = [args.data+'pathsigns.train', \n",
    "                    args.data+'pathsigns.test']\n",
    "\n",
    "# Processing sentences and paths\n",
    "preprocessed_sentences, vocab = preprocessing_sentences(paths_translation, max_len=args.max_len_sentence)\n",
    "preprocessed_paths = preprocessing_paths(paths_videos, path2videos=args.videos_path, type_=args.type_frames)\n",
    "\n",
    "#Creating table paths\n",
    "table_paths=table_paths_dataset(preprocessed_paths, preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_augmentation import frame_sampling\n",
    "\n",
    "raw_data = flow_from_tablePaths(table_paths, lambda x: x, args.inputShape[1:3])\n",
    "\n",
    "def train_gen_sampling():\n",
    "    train_gen = raw_data.data_generator(1, args.inputShape[-1])\n",
    "    for v, l in train_gen:\n",
    "        s = np.r_[[int(j) for j in (raw_data.to_class[l]).split(\", \")]]\n",
    "        for new_v in frame_sampling(v, args.inputShape[0]):\n",
    "            yield (new_v, s[:-1]), s[1:]\n",
    "train_data = tf.data.Dataset.from_generator(train_gen_sampling, ((tf.float32, tf.int64), tf.int64),\n",
    "    ((args.inputShape, args.max_len_sentence-1), args.max_len_sentence-1))\n",
    "\n",
    "def test_gen_sampling():\n",
    "    test_gen = raw_data.data_generator(2, args.inputShape[-1])\n",
    "    for v, l in test_gen:\n",
    "        s = np.r_[[int(j) for j in (raw_data.to_class[l]).split(\", \")]]\n",
    "        for new_v in frame_sampling(v, args.inputShape[0]):\n",
    "            yield (new_v, s[:-1]), s[1:]\n",
    "test_data = tf.data.Dataset.from_generator(test_gen_sampling, ((tf.float32, tf.int64), tf.int64),\n",
    "    ((args.inputShape, args.max_len_sentence-1), args.max_len_sentence-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entradas de la red\n",
    "input_video = tf.keras.Input(shape=args.inputShape, name=\"input_video\")\n",
    "input_words = tf.keras.Input(shape=[args.max_len_sentence-1], name=\"input_words\")\n",
    "\n",
    "# Compute features and reduce features\n",
    "x = compute_features.compute_features_v1_0(input_video, weight_decay=tf.keras.regularizers.l2(args.wDecay))\n",
    "x = reduce_features.reduce_features_v1_2(x)\n",
    "\n",
    "#Encoder module and self attention\n",
    "x1, rnn1_states, rnn2_states = encoder.encoder_v1_1(x, args.rnnUnits, args.unitsEmbedding, \n",
    "    args.dropout, args.recurrent_dropout)\n",
    "x1 = self_attentions.self_attention_v1_0(x1)\n",
    "\n",
    "#Decoder module\n",
    "x2 = decoder.decoder_v1_0(input_words, rnn1_states, rnn2_states, args.rnnUnits, args.unitsEmbedding, \n",
    "    args.vocab_size, args.dropout, args.recurrent_dropout)\n",
    "\n",
    "# Spatio Temporal attention\n",
    "x3 = st_attentions.st_attention_v1_4_1(x2, x1, x)\n",
    "\n",
    "# Output of the network\n",
    "x = output.output_v1_0(x2, x3, args.vocab_size)\n",
    "\n",
    "model = tf.keras.Model(inputs=(input_video, input_words), outputs=x, name=args.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model, to_file=args.name+'.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = args.lr\n",
    "    drop = args.decay\n",
    "    epochs_drop = args.nIters\n",
    "    lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "calls = [tf.keras.callbacks.LearningRateScheduler(step_decay, verbose=1),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.optimizer == 'adam':\n",
    "    opt = tf.keras.optimizers.Adam(\n",
    "        lr=args.lr, \n",
    "        beta_1=0.9, \n",
    "        beta_2=0.999, \n",
    "        epsilon=1e-08, \n",
    "        decay=0.0, \n",
    "        clipnorm=1., \n",
    "        clipvalue=5)\n",
    "\n",
    "elif args.optimizer == 'sgd':\n",
    "    opt = tf.keras.optimizers.SGD(\n",
    "        lr=args.lr, \n",
    "        decay=0, \n",
    "        momentum=args.momentum, \n",
    "        nesterov=True, \n",
    "        clipnorm=1., \n",
    "        clipvalue=0.5)\n",
    "\n",
    "elif args.optimizer == 'rsmprop':\n",
    "    opt = tf.keras.optimizers.RMSprop(lr=args.lr) \n",
    "                         #clipnorm=1., \n",
    "                         #clipvalue=0.5)      \n",
    "else:\n",
    "    raise ValueError('You must specify a valid optimizer for model. The only optmizers available are: '\n",
    "                    '\"adam\", \"sgd\" or \"rmsprop\". The optmizer given was: '+str(args.optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformations and augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(data, label):\n",
    "    return (data[0]/255., data[1]), label\n",
    "\n",
    "train_data = train_data.cache().map(scale, 24)\n",
    "train_data = train_data.shuffle(318, reshuffle_each_iteration=True).batch(args.batchSize)\n",
    "train_data = train_data.prefetch(args.prefetch_batch_buffer)\n",
    "\n",
    "test_data = test_data.cache().map(scale,24)\n",
    "test_data = test_data.shuffle(84, reshuffle_each_iteration=True).batch(args.batchSize).prefetch(args.prefetch_batch_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt, loss=SparseCategoricalCrossentropy_mask, metrics=[real_acc,\n",
    "    tf.keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x = train_data, \n",
    "          epochs=args.epochs,\n",
    "          callbacks=calls,\n",
    "          validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(args.path2save, \"trained_model.h5\"), include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning (Optional to the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Boston dataset doesn't have dev dision so there is not finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen_sampling():\n",
    "    i = 0\n",
    "    train_gen = raw_data.data_generator(1, args.inputShape[-1])\n",
    "    train = table_paths[table_paths[:,1] == \"train\"]\n",
    "    \n",
    "    for v, l in train_gen:\n",
    "        s = np.r_[[int(j) for j in (raw_data.to_class[l]).split(\", \")]]\n",
    "        p = train[i][0]\n",
    "        i += 1\n",
    "        for new_v in [frame_sampling(v, args.inputShape[0])[0]]:\n",
    "            yield new_v, s, p\n",
    "train_data = tf.data.Dataset.from_generator(train_gen_sampling, (tf.float32, tf.int64, tf.string)).batch(1)\n",
    "\n",
    "def test_gen_sampling():\n",
    "    i = 0\n",
    "    test_gen = raw_data.data_generator(2, args.inputShape[-1])\n",
    "    test = table_paths[table_paths[:,1] == \"test\"]\n",
    "    for v, l in test_gen:\n",
    "        s = np.r_[[int(j) for j in (raw_data.to_class[l]).split(\", \")]]\n",
    "        p = test[i][0]\n",
    "        i += 1\n",
    "        for new_v in [frame_sampling(v, args.inputShape[0])[0]]:\n",
    "            yield new_v, s, p\n",
    "test_data = tf.data.Dataset.from_generator(test_gen_sampling, (tf.float32, tf.int64, tf.string)).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = save_predictions(model, args.path2save, vocab, args, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "calculate_metrics_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
