{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuraciones iniciales (Grafica y Equipo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# lA ID de la GPU a usar, puede ser desde 0 hasta las N GPU's. Si es -1 significa que es en la CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\";\n",
    "\n",
    "#Si se esta trabajando en colab la variable debe quedar en falso\n",
    "SERVER = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instaladas python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import interact, IntSlider\n",
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "if not SERVER:\n",
    "    %tensorflow_version 2.x\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propias y externas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SERVER:\n",
    "    #Archivos py\n",
    "    !git clone https://github.com/JefeLitman/SignLanguageTranslation_SLT.git\n",
    "    #Organizacion archivos\n",
    "    !mv /content/SignLanguageTranslation_SLT/utils /content/\n",
    "    !mv /content/SignLanguageTranslation_SLT/models /content/\n",
    "    #Elimino la carpeta sobrante\n",
    "    !rm -rf /content/SignLanguageTranslation_SLT\n",
    "#DatasetsLoaderUtils\n",
    "!wget -q https://raw.githubusercontent.com/JefeLitman/VideoDataGenerator/master/DatasetsLoaderUtils.py -O DatasetsLoaderUtils.py\n",
    "!mv DatasetsLoaderUtils.py utils/DatasetsLoaderUtils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.DatasetsLoaderUtils import flow_from_tablePaths\n",
    "from utils.preprocess_data import preprocessing_paths, preprocessing_sentences, table_paths_dataset\n",
    "from utils.data_augmentation import frame_sampling\n",
    "from utils.build_tf_data import build_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SERVER:\n",
    "    !rm -rf /content/sample_data\n",
    "    !wget --quiet --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Ph_Ys3O_vI93WeTkDqr5h6kTJm0CZ0Ub' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Ph_Ys3O_vI93WeTkDqr5h6kTJm0CZ0Ub\" -O boston201.zip && rm -rf /tmp/cookies.txt\n",
    "    !unzip -q boston201.zip\n",
    "    !wget -q https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip -O word_vectors.zip\n",
    "    !unzip -q word_vectors.zip\n",
    "    # Monto el drive para poder tener donde guardar los datos\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo SLT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametros de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = namedtuple('Args', 'tx ty data loss pretrained prefetch_batch_buffer dataset verbose unitsSatt unitsEmbedding vocab_size rol videos_path rnnUnits denseUnits dropout recurrent_dropout inputShape optimizer type_frames batchSize epochs nIters lr momentum decay wDecay path2save name')\n",
    "\n",
    "args = Args(tx=32, \n",
    "            ty=15,\n",
    "            videos_path='../DataSets/boston201',#'/content/boston201',\n",
    "            denseUnits=256,\n",
    "            rnnUnits=256,\n",
    "            unitsEmbedding=300,\n",
    "            unitsSatt=169,\n",
    "            vocab_size=150,\n",
    "            loss ='SparseCategoricalCrossentropy_mask',\n",
    "            rol = 'train',\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2,\n",
    "            inputShape=(32, 112, 112, 3),\n",
    "            pretrained=None,#'vgg16',\n",
    "            optimizer='adam',\n",
    "            type_frames='jpg/',\n",
    "            batchSize=1,\n",
    "            epochs=10,\n",
    "            nIters=10.0,\n",
    "            lr=0.001,\n",
    "            momentum=0.99,\n",
    "            decay=0.1,\n",
    "            wDecay=0.0005,\n",
    "            path2save='path_to_save_model_results',\n",
    "            name='SLT_Model',\n",
    "            data= '../DataSets/boston201/data/', #'/content/boston201/data/',\n",
    "            dataset='boston201',\n",
    "            prefetch_batch_buffer = 5,\n",
    "            verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparacion datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training file ...\n",
      "training on train sentences to make the vocab ...\n",
      "training text to index sequences ..\n",
      "training index sequences to padded sequences ..\n",
      "Computing same steps over dev and test sequences\n",
      "Completed\n",
      "Completed\n",
      "Creating the table paths for flow_from_tablePaths ..\n"
     ]
    }
   ],
   "source": [
    "paths_translation = [args.data+'translations.train',  \n",
    "                         args.data+'translations.test']\n",
    "paths_videos = [args.data+'pathsigns.train', \n",
    "                    args.data+'pathsigns.test']\n",
    "\n",
    "# Processing sentences and paths\n",
    "preprocessed_sentences, vocab = preprocessing_sentences(paths_translation, max_len=args.ty)\n",
    "preprocessed_paths = preprocessing_paths(paths_videos, path2videos=args.videos_path, type_=args.type_frames)\n",
    "\n",
    "#Creating table paths\n",
    "table_paths=table_paths_dataset(preprocessed_paths, preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = flow_from_tablePaths(table_paths, lambda x: x, args.inputShape[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = dataset.data_generator(1,args.inputShape[-1])\n",
    "test_gen = dataset.data_generator(2,args.inputShape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_paths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76, 112, 112, 3)\n"
     ]
    }
   ],
   "source": [
    "for video, label in test_gen:\n",
    "    print(video.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen_sampling():\n",
    "    for v, l in train_gen:\n",
    "        for new_v in frame_sampling(v, args.inputShape[0]):\n",
    "            yield new_v, l\n",
    "def test_gen_sampling():\n",
    "    for v, l in test_gen:\n",
    "        for new_v in frame_sampling(v, args.inputShape[0]):\n",
    "            yield new_v, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frame_sampling(video, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 112, 112, 3)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_sampling(video, 32)[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion y estructuracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
